<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>AndersGee's blog</title>
    <link rel="icon" href="../../launcher-icon-4x.png">
    <link rel="stylesheet" href="../../css/materialize.css">
    <link rel="stylesheet" href="../../css/topnav.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
    img {
      width: 100%;
      height: 100%;
      overflow: hidden;
    }
    </style>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
    });
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  </head>

  <body>
    <ul class="topnav">
      <div class="container">
        <li><a href="../../index.html">Home</a></li>
        <li class="right"><a href="../../contact.html">Contact</a></li>
        <li class="right"><a href="../../about.html">About</a></li>
      </div>
    </ul>

    <div class="container">

      <div class="section"><h5><center>3d Grid LSTM</center></h5>
        <p>
          Visualization of the gradients while a 3d Grid LSTM [1] is learning.
          The colors are red green and blue in the three respective dimensions
          and the size of the balls represent the magnitude of the gradient. For example: a big yellow ball would mean
          the gradients are big in dimensions 1 and 2 (red, green) and not so big in dimension 3 (blue).
          I only feed input in dimension 1 of the input cell (This is why the ball at the top corner of the cube is red) The output cell is in the opposite corner.
        </p>
        <p>
          Note that for the purpose of this visualization, a randomly generated but constant input vector $x$ and target vector $t$ are used.
          The Grid LSTM thus learns the function $t=f(x)$ for a constant $x$ and $t$ which is the simplest possible version of any neural net architecture.
        </p>
        <div class="videowrapper"><iframe src="https://www.youtube.com/embed/zicZXGd_JAI" frameborder="0" allowfullscreen></iframe></div>
        <p>
          The learning scheme is Adam but with a learning rate about 10x higher than
          normal, meaning that parameters are over-adjusted and oscillates to the
          minimum instead of "slowing momentum when getting closer to the minimum".
          I looks natural enough, and I think its because the human body always seem to over compensate
          in response to external input.
        </p>
        <p>
          The below video shows one way real neuronal activity can be visualized.
          Play both videos at the same time to see some similarity.
        </p>
        <div class="videowrapper"><iframe src="https://www.youtube.com/embed/t3TaMU_qXMc?start=16&end=40" frameborder="0" allowfullscreen></iframe></div>
      </div>

      <div class="divider"></div>

      <div class="section">
        <p>
          References: <br>

        [1]. <a href="https://arxiv.org/abs/1507.01526">
          Grid Long Short-Term Memory (Nal Kalchbrenner, Ivo Danihelka, Alex Graves, 2016)</a>
        </p>
      </div>

      <div class="divider"></div>

    </div>
  </body>
</html>
